"""
AI-Router-Lite: è½»é‡çº§ AI èšåˆè·¯ç”±

ä¸»åº”ç”¨å…¥å£
"""

import sys
import time
from contextlib import asynccontextmanager

import uvicorn
from colorama import init as colorama_init, Fore, Style
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse

from src.config import config_manager, get_config
from src.models import (
    ChatCompletionRequest,
    ErrorResponse,
    ErrorDetail,
    ModelListResponse,
    ModelInfo,
)
from src.provider import provider_manager
from src.router import ModelRouter
from src.proxy import RequestProxy, ProxyError


# åˆå§‹åŒ– colorama
colorama_init()


# å…¨å±€ç»„ä»¶
router: ModelRouter = None  # type: ignore
proxy: RequestProxy = None  # type: ignore


def print_banner():
    """æ‰“å°å¯åŠ¨æ¨ªå¹…"""
    banner = f"""
{Fore.CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                          â•‘
â•‘   {Fore.WHITE}ğŸš€ AI-Router-Lite v0.3.0{Fore.CYAN}                              â•‘
â•‘   {Fore.WHITE}è½»é‡çº§ AI èšåˆè·¯ç”±{Fore.CYAN}                                    â•‘
â•‘                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Style.RESET_ALL}
"""
    print(banner)


def print_config_summary():
    """æ‰“å°é…ç½®æ‘˜è¦"""
    config = get_config()
    print(f"{Fore.GREEN}[CONFIG]{Style.RESET_ALL} æœåŠ¡åœ°å€: http://{config.server_host}:{config.server_port}")
    print(f"{Fore.GREEN}[CONFIG]{Style.RESET_ALL} æœ€å¤§é‡è¯•æ¬¡æ•°: {config.max_retries}")
    print(f"{Fore.GREEN}[CONFIG]{Style.RESET_ALL} è¯·æ±‚è¶…æ—¶: {config.request_timeout}s")
    print(f"{Fore.GREEN}[CONFIG]{Style.RESET_ALL} æ¨¡å‹æ˜ å°„: {len(config.model_map)} ä¸ª")
    print(f"{Fore.GREEN}[CONFIG]{Style.RESET_ALL} Provider æ•°é‡: {len(config.providers)} ä¸ª")
    
    for p in config.providers:
        print(f"  {Fore.CYAN}â”œâ”€{Style.RESET_ALL} {p.name} (æƒé‡: {p.weight}, æ¨¡å‹: {len(p.supported_models)} ä¸ª)")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global router, proxy
    
    # å¯åŠ¨æ—¶
    print_banner()
    
    try:
        config = config_manager.load("config.json")
        print(f"{Fore.GREEN}[STARTUP]{Style.RESET_ALL} é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    except FileNotFoundError:
        print(f"{Fore.RED}[ERROR]{Style.RESET_ALL} é…ç½®æ–‡ä»¶ config.json ä¸å­˜åœ¨ï¼")
        print(f"{Fore.YELLOW}[HINT]{Style.RESET_ALL} è¯·å¤åˆ¶ config.example.json å¹¶é‡å‘½åä¸º config.json")
        sys.exit(1)
    except Exception as e:
        print(f"{Fore.RED}[ERROR]{Style.RESET_ALL} é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    # æ³¨å†Œ Provider
    provider_manager.register_all(config.providers)
    
    # åˆå§‹åŒ–è·¯ç”±å™¨å’Œä»£ç†
    router = ModelRouter(config, provider_manager)
    proxy = RequestProxy(config, provider_manager, router)
    
    print_config_summary()
    print(f"{Fore.GREEN}[STARTUP]{Style.RESET_ALL} æœåŠ¡å¯åŠ¨å®Œæˆï¼Œç­‰å¾…è¯·æ±‚...")
    print("-" * 60)
    
    yield
    
    # å…³é—­æ—¶
    await proxy.close()
    print(f"{Fore.YELLOW}[SHUTDOWN]{Style.RESET_ALL} æœåŠ¡å·²å…³é—­")


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="AI-Router-Lite",
    description="è½»é‡çº§ AI èšåˆè·¯ç”±",
    version="0.3.0",
    lifespan=lifespan
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== API ç«¯ç‚¹ ====================


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "AI-Router-Lite",
        "version": "0.3.0",
        "status": "running"
    }


@app.get("/v1/models")
async def list_models():
    """
    åˆ—å‡ºå¯ç”¨æ¨¡å‹ (OpenAI å…¼å®¹)
    """
    models = router.get_available_models()
    return ModelListResponse(
        data=[
            ModelInfo(
                id=model,
                created=int(time.time())
            )
            for model in models
        ]
    )


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest, raw_request: Request):
    """
    èŠå¤©è¡¥å…¨ç«¯ç‚¹ (OpenAI å…¼å®¹)
    
    æ”¯æŒæµå¼å’Œéæµå¼å“åº”
    """
    original_model = request.model
    is_stream = request.stream or False
    
    # è®°å½•è¯·æ±‚æ—¥å¿—
    print(
        f"{Fore.MAGENTA}[REQUEST]{Style.RESET_ALL} "
        f"æ¨¡å‹: {original_model}, æµå¼: {is_stream}, "
        f"æ¶ˆæ¯æ•°: {len(request.messages)}"
    )
    
    try:
        if is_stream:
            # æµå¼å“åº”
            return StreamingResponse(
                proxy.forward_stream(request, original_model),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no"  # ç¦ç”¨ nginx ç¼“å†²
                }
            )
        else:
            # éæµå¼å“åº”
            response = await proxy.forward_request(request, original_model)
            return JSONResponse(content=response)
            
    except ProxyError as e:
        print(f"{Fore.RED}[ERROR]{Style.RESET_ALL} ä»£ç†é”™è¯¯: {e.message}")
        status_code = e.status_code or 500
        return JSONResponse(
            status_code=status_code,
            content=ErrorResponse(
                error=ErrorDetail(
                    message=e.message,
                    type="proxy_error",
                    code=str(status_code)
                )
            ).model_dump()
        )
    except Exception as e:
        print(f"{Fore.RED}[ERROR]{Style.RESET_ALL} æœªçŸ¥é”™è¯¯: {str(e)}")
        return JSONResponse(
            status_code=500,
            content=ErrorResponse(
                error=ErrorDetail(
                    message=f"å†…éƒ¨é”™è¯¯: {str(e)}",
                    type="internal_error",
                    code="500"
                )
            ).model_dump()
        )


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    stats = provider_manager.get_stats()
    return {
        "status": "healthy",
        "available_providers": stats["available_providers"],
        "total_providers": stats["total_providers"]
    }


@app.get("/stats")
async def get_stats():
    """è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
    return provider_manager.get_stats()


@app.post("/admin/reset/{provider_name}")
async def reset_provider(provider_name: str):
    """é‡ç½®æŒ‡å®š Provider çš„çŠ¶æ€"""
    if provider_manager.reset(provider_name):
        return {"status": "success", "message": f"Provider '{provider_name}' å·²é‡ç½®"}
    else:
        raise HTTPException(status_code=404, detail=f"Provider '{provider_name}' ä¸å­˜åœ¨")


@app.post("/admin/reset-all")
async def reset_all_providers():
    """é‡ç½®æ‰€æœ‰ Provider çš„çŠ¶æ€"""
    provider_manager.reset_all()
    return {"status": "success", "message": "æ‰€æœ‰ Provider å·²é‡ç½®"}


# ==================== ä¸»å…¥å£ ====================


if __name__ == "__main__":
    # å…ˆå°è¯•åŠ è½½é…ç½®ä»¥è·å–ç«¯å£
    try:
        config = config_manager.load("config.json")
        host = config.server_host
        port = config.server_port
    except:
        host = "0.0.0.0"
        port = 8000
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=False,
        log_level="info"
    )